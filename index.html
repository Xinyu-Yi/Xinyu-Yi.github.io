<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Xinyu Yi</title>
    <link href="css/bootstrap-4.3.1.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
  </head>
  <body>
    <div class="container">
      
      <!-- Bio -->
      <div class="row mt-4">
        <div class="col-12 text-center"> <h1>Xinyu Yi</h1> </div>
      </div>
      <hr>
      <div class="row">
        <div class="col-md-9 col-sm-12">
          <div class="media">
            <img class="mr-3" src="images/yxy-formal.jpg" style="height: 140px;" alt="Xinyu Yi">
            <!--img class="mr-3 rounded-circle" src="images/yxy.jpg" style="height: 150px;" alt="Xinyu Yi"-->
            <div class="media-body mt-1">
              <p>I'm a Research Scientist at ByteDance, focusing on robotics research. 
                 Previously, I obtained my Ph.D. from Tsinghua University in 2025, supervised by Prof. <a href="http://xufeng.site/">Feng Xu</a>. 
                 Before that, I received my B.S. from School of the Gifted Young, University of Science and Technology of China in 2020.</p>
              <p>My research aims to build physics-based human motion capture and human-environment interaction systems.
                 During my Ph.D., I worked extensively on real-time human motion capture from sparse lightweight body-worn sensors. </p>
            </div>
          </div>
        </div>
        <div class="col-md-3 col-sm-12">
          <div class="media">
            <div class="media-body">
              <ul class="fa-ul">
                <!--li> <i class="fa-li fa fa-map-marker"></i> <a href="https://www.tsinghua.edu.cn/">Tsinghua University</a>, China </li-->
                <li> <i class="fa-li fa fa-file-pdf-o"></i> <a href="files/cv-yxy-en.pdf" target="_blank">English CV</a> | <a href="files/cv-yxy-zh.pdf" target="_blank">中文CV</a> </li>
                <li> <i class="fa-li fa fa-envelope"></i> <a href="mailto:yixy20@tsinghua.org.cn">yixy20@tsinghua.org.cn</a> </li> 
                <li> <i class="fa-li fa fa-phone"></i> +86 17305697122 </li>
                <li> <i class="fa-li fa fa-github"></i> <a href="https://github.com/Xinyu-Yi">GitHub</a> </li>
                <li> <i class="fa-li fa fa-graduation-cap"></i> <a href="https://scholar.google.com/citations?user=C_46RHIAAAAJ">Google Scholar</a> </li>
                <li> <i class="fa-li fa fa-pencil"></i> <a href="https://xinyu-yi.github.io/Blogs/">Ideas &amp; Notes</a> </li>
              </ul>
            </div>
          </div>
        </div>
      </div>

      <!-- Research -->
      <div class="row mt-4">
        <div class="col-md-12 col-sm-12">
          <h2>Research</h2>
          <hr>

          <!--div class="media mt-3">
            <img class="mr-3 img-thumbnail" style="height:150px" src="images/SAIP.jpg" alt="SAIP">
            <div class="media-body">
              <h6 class="mt-0">Shape-aware Inertial Poser: Motion Tracking for Humans with Diverse Shapes Using Sparse Inertial Sensors</h6>
              <p><strong>Xinyu Yi</strong><br>
                 <em>International Conference on Computer Vision (<font color="red"><strong>ICCV</strong></font>), 2025</em></p>
              <ul class="list-inline">
                <li class="list-inline-item"><a href="https://arxiv.org/abs/2506.22907">Paper</a></li>
                <li class="list-inline-item"><a href="https://github.com/Xinyu-Yi/GlobalPose">Code</a></li>
                <li class="list-inline-item"><a href="https://www.youtube.com/watch?v=2kBgIQgnS3g">Video</a></li>
                <li class="list-inline-item"><a href="https://xinyu-yi.github.io/GlobalPose/">Project page</a></li>
              </ul>
            </div>
          </div-->

          <div class="media mt-3">
            <img class="mr-3 img-thumbnail" style="height:150px" src="images/DiffCap.jpg" alt="DiffCap">
            <div class="media-body">
              <h6 class="mt-0">DiffCap: Diffusion-based Real-time Human Motion Capture using Sparse IMUs and a Monocular Camera</h6>
              <p>Shaohua Pan, <strong>Xinyu Yi</strong>, Yan Zhou, Weihua Jian, Yuan Zhang, Pengfei Wan, Feng Xu<br>
                 <em>Transactions on Visualization and Computer Graphics (<font color="red"><strong>TVCG</strong></font>), 2025</em></p>
              <ul class="list-inline">
                <li class="list-inline-item"><a href="https://arxiv.org/abs/2508.06139">Paper</a></li>
                <!--li class="list-inline-item"><a href="https://github.com/Xinyu-Yi/GlobalPose">Code</a></li-->
                <!--li class="list-inline-item"><a href="https://www.youtube.com/watch?v=2kBgIQgnS3g">Video</a></li-->
                <!--li class="list-inline-item"><a href="https://xinyu-yi.github.io/GlobalPose/">Project page</a></li-->
              </ul>
            </div>
          </div>

          <div class="media mt-4">
            <img class="mr-3 img-thumbnail" style="height:150px" src="images/MagShield.jpg" alt="MagShield">
            <div class="media-body">
              <h6 class="mt-0">MagShield: Towards Better Robustness in Sparse Inertial Motion Capture Under Magnetic Disturbances</h6>
              <p>Yunzhe Shao, <strong>Xinyu Yi</strong>, Lu Yin, Shihui Guo, Junhai Yong, Feng Xu<br>
                 <em>International Conference on Computer Vision (<font color="red"><strong>ICCV</strong></font>), 2025</em></p>
              <ul class="list-inline">
                <li class="list-inline-item"><a href="https://arxiv.org/abs/2506.22907">Paper</a></li>
                <!--li class="list-inline-item"><a href="https://github.com/Xinyu-Yi/GlobalPose">Code</a></li-->
                <!--li class="list-inline-item"><a href="https://www.youtube.com/watch?v=2kBgIQgnS3g">Video</a></li-->
                <!--li class="list-inline-item"><a href="https://xinyu-yi.github.io/GlobalPose/">Project page</a></li-->
              </ul>
            </div>
          </div>
      
          <div class="media mt-4">
            <img class="mr-3 img-thumbnail" style="height:150px" src="images/BaroPoser.jpg" alt="BaroPoser">
            <div class="media-body">
              <h6 class="mt-0">BaroPoser: Real-time Human Motion Tracking from IMUs and Barometers in Everyday Devices</h6>
              <p>Libo Zhang, <strong>Xinyu Yi</strong>, Feng Xu<br>
                 <em>ACM Symposium on User Interface Software and Technology (<font color="red"><strong>UIST</strong></font>), 2025</em></p>
              <ul class="list-inline">
                <li class="list-inline-item"><a href="https://arxiv.org/abs/2508.03313">Paper</a></li>
                <!--li class="list-inline-item"><a href="https://github.com/Xinyu-Yi/GlobalPose">Code</a></li-->
                <!--li class="list-inline-item"><a href="https://www.youtube.com/watch?v=2kBgIQgnS3g">Video</a></li-->
                <!--li class="list-inline-item"><a href="https://xinyu-yi.github.io/GlobalPose/">Project page</a></li-->
              </ul>
            </div>
          </div>

          <div class="media mt-4">
            <img class="mr-3 img-thumbnail" style="height:150px" src="images/GlobalPose.jpg" alt="GlobalPose">
            <div class="media-body">
              <h6 class="mt-0">Improving Global Motion Estimation in Sparse IMU-based Motion Capture with Physics</h6>
              <p><strong>Xinyu Yi</strong>, Shaohua Pan, Feng Xu<br>
                 <em>ACM Transactions on Graphics (Proc. of <font color="red"><strong>SIGGRAPH</strong></font>), 2025</em></p>
              <ul class="list-inline">
                <li class="list-inline-item"><a href="https://arxiv.org/abs/2505.05010">Paper</a></li>
                <li class="list-inline-item"><a href="https://github.com/Xinyu-Yi/GlobalPose">Code</a></li>
                <li class="list-inline-item"><a href="https://www.youtube.com/watch?v=2kBgIQgnS3g">Video</a></li>
                <li class="list-inline-item"><a href="https://xinyu-yi.github.io/GlobalPose/">Project page</a></li>
              </ul>
            </div>
          </div>
      
          <div class="media mt-4">
            <img class="mr-3 img-thumbnail" style="height:150px" src="images/TIC.jpg" alt="TIC">
            <div class="media-body">
              <h6 class="mt-0">Transformer IMU Calibrator: Dynamic On-body IMU Calibration for Inertial Motion Capture</h6>
              <p>Chengxu Zuo, Jiawei Huang, Xiao Jiang, Yuan Yao, Xiangren Shi, Rui Cao, <strong>Xinyu Yi</strong>, Feng Xu, Shihui Guo, Yipeng Qin<br>
                 <em>ACM Transactions on Graphics (Proc. of <font color="red"><strong>SIGGRAPH</strong></font>), 2025 (<font color="red"><strong>Best Paper Award</strong></font>)</em></p>
              <ul class="list-inline">
                <li class="list-inline-item"><a href="https://orca.cardiff.ac.uk/id/eprint/177840/1/TIC_camera_ready.pdf">Paper</a></li>
                <li class="list-inline-item"><a href="https://github.com/ZuoCX1996/TIC">Code</a></li>
                <li class="list-inline-item"><a href="https://youtu.be/xu2v_yPJoLs">Video</a></li>
                <li class="list-inline-item"><a href="https://www.humanplus.xyz/siggraph-2025-zcx">Project page</a></li>
              </ul>
            </div>
          </div>
      
          <div class="media mt-4">
            <img class="mr-3 img-thumbnail" style="height:150px" src="images/PNP.jpg" alt="PNP">
            <div class="media-body">
              <h6 class="mt-0">Physical Non-inertial Poser (PNP): Modeling Non-inertial Effects in Sparse-inertial Human Motion Capture</h6>
              <p><strong>Xinyu Yi</strong>, Yuxiao Zhou, Feng Xu<br>
                 <em>Proc. of <font color="red"><strong>SIGGRAPH</strong></font>, 2024</em></p>
              <ul class="list-inline">
                <li class="list-inline-item"><a href="https://arxiv.org/abs/2404.19619">Paper</a></li>
                <li class="list-inline-item"><a href="https://github.com/Xinyu-Yi/PNP">Code</a></li>
                <li class="list-inline-item"><a href="https://www.youtube.com/watch?v=c10-KvBoZMA">Video(demo)</a></li>
                <li class="list-inline-item"><a href="https://www.youtube.com/watch?v=NdbOtA7Rns0">Video(method)</a></li>
                <li class="list-inline-item"><a href="https://xinyu-yi.github.io/PNP/">Project page</a></li>
              </ul>
            </div>
          </div>
      
          <div class="media mt-4">
            <img class="mr-3 img-thumbnail" style="height:150px" src="images/HOIC.jpg" alt="HOIC">
            <div class="media-body">
              <h6 class="mt-0">Hand-Object Interaction Controller (HOIC): Deep Reinforcement Learning for Reconstructing Interactions with Physics</h6>
              <p>Haoyu Hu, <strong>Xinyu Yi</strong>, Zhe Cao, Jun-Hai Yong, Feng Xu<br>
                 <em>Proc. of <font color="red"><strong>SIGGRAPH</strong></font>, 2024</em></p>
              <ul class="list-inline">
                <li class="list-inline-item"><a href="https://arxiv.org/abs/2405.02676">Paper</a></li>
                <li class="list-inline-item"><a href="https://github.com/hu-hy17/HOIC">Code</a></li>
                <li class="list-inline-item"><a href="https://www.youtube.com/watch?v=_-aqtfTy5v4">Video</a></li>
                <!--li class="list-inline-item"><a href="https://shaohua-pan.github.io/robustcap-page/">Project page</a></li-->
              </ul>
            </div>
          </div>
      
          <div class="media mt-4">
            <img class="mr-3 img-thumbnail" style="height:150px" src="images/LIP.jpg" alt="LIP">
            <div class="media-body">
              <h6 class="mt-0">Loose Inertial Poser: Motion Capture with IMU-attached Loose-Wear Jacket</h6>
              <p>Chengxu Zuo, Yiming Wang, Lishuang Zhan, Shihui Guo, <strong>Xinyu Yi</strong>, Feng Xu, Yipeng Qin<br>
                 <em>Computer Vision and Pattern Recognition (<font color="red"><strong>CVPR</strong></font>), 2024</em></p>
              <ul class="list-inline">
                <li class="list-inline-item"><a href="https://orca.cardiff.ac.uk/id/eprint/167381/1/%5Bcamera-ready%5D%20LIP.pdf">Paper</a></li>
                <li class="list-inline-item"><a href="https://github.com/ZuoCX1996/Loose-Inertial-Poser">Code</a></li>
                <li class="list-inline-item"><a href="https://www.bilibili.com/video/BV1aB421z7xE/">Video</a></li>
                <li class="list-inline-item"><a href="https://www.humanplus.xyz/cvpr2024-zcx">Project page</a></li-->
              </ul>
            </div>
          </div>
      
          <div class="media mt-4">
            <img class="mr-3 img-thumbnail" style="height:150px" src="images/RobustCap.jpg" alt="RobustCap">
            <div class="media-body">
              <h6 class="mt-0">Fusing Monocular Images and Sparse IMU Signals for Real-time Human Motion Capture</h6>
              <p>Shaohua Pan, Qi Ma, <strong>Xinyu Yi</strong>, Weifeng Hu, Xiong Wang, Xingkang Zhou, Jijunnan Li, Feng Xu<br>
                 <em>Proc. of <font color="red"><strong>SIGGRAPH Asia</strong></font>, 2023</em></p>
              <ul class="list-inline">
                <li class="list-inline-item"><a href="https://arxiv.org/abs/2309.00310">Paper</a></li>
                <li class="list-inline-item"><a href="https://github.com/shaohua-pan/RobustCap">Code</a></li>
                <li class="list-inline-item"><a href="https://www.youtube.com/watch?v=aqzvWNSU4JM">Video</a></li>
                <li class="list-inline-item"><a href="https://shaohua-pan.github.io/robustcap-page/">Project page</a></li>
              </ul>
            </div>
          </div>
      
          <div class="media mt-4">
            <img class="mr-3 img-thumbnail" style="height:150px" src="images/EgoLocate.jpg" alt="EgoLocate">
            <div class="media-body">
              <h6 class="mt-0">EgoLocate: Real-time Motion Capture, Localization, and Mapping with Sparse Body-mounted Sensors</h6>
              <p><strong>Xinyu Yi</strong>, Yuxiao Zhou, Marc Habermann, Vladislav Golyanik, Shaohua Pan, Christian Theobalt, Feng Xu<br>
                 <em>ACM Transactions on Graphics (Proc. of <font color="red"><strong>SIGGRAPH</strong></font>), 2023</em></p>
              <ul class="list-inline">
                <li class="list-inline-item"><a href="https://arxiv.org/abs/2305.01599">Paper</a></li>
                <li class="list-inline-item"><a href="https://github.com/Xinyu-Yi/EgoLocate">Code</a></li>
                <li class="list-inline-item"><a href="https://www.youtube.com/watch?v=URxUtzkF5Uk">Video</a></li>
                <li class="list-inline-item"><a href="https://www.youtube.com/watch?v=QiSEduWxT3s">Talk</a></li>
                <li class="list-inline-item"><a href="https://xinyu-yi.github.io/EgoLocate/">Project page</a></li>
              </ul>
            </div>
          </div>
      
          <div class="media mt-4">
            <img class="mr-3 img-thumbnail" style="height:150px" src="images/PIP.jpg" alt="PIP">
            <div class="media-body">
              <h6 class="mt-0">Physical Inertial Poser (PIP): Physics-aware Real-time Human Motion Tracking from Sparse Inertial Sensors</h6>
              <p><strong>Xinyu Yi</strong>, Yuxiao Zhou, Marc Habermann, Soshi Shimada, Vladislav Golyanik, Christian Theobalt, Feng Xu<br>
                 <em>Computer Vision and Pattern Recognition (<font color="red"><strong>CVPR</strong></font>), 2022 (<font color="red"><strong>Best Paper Finalist</strong></font>)</em></p>
              <ul class="list-inline">
                <li class="list-inline-item"><a href="https://arxiv.org/abs/2203.08528">Paper</a></li>
                <li class="list-inline-item"><a href="https://github.com/Xinyu-Yi/PIP">Code</a></li>
                <li class="list-inline-item"><a href="https://www.youtube.com/watch?v=KTqj2a3lKo0">Video</a></li>
                <li class="list-inline-item"><a href="https://www.bilibili.com/video/BV1nP411E7cf/">Talk(Chinese)</a></li>
                <li class="list-inline-item"><a href="https://xinyu-yi.github.io/PIP/">Project page</a></li>
              </ul>
            </div>
          </div>
      
          <div class="media mt-4">
            <img class="mr-3 img-thumbnail" style="height:150px" src="images/PhysInteraction.jpg" alt="PhysInteraction">
            <div class="media-body">
              <h6 class="mt-0">Physical Interaction: Reconstructing Hand-object Interactions with Physics</h6>
              <p>Haoyu Hu, <strong>Xinyu Yi</strong>, Hao Zhang, Jun-Hai Yong, Feng Xu<br>
                 <em>Proc. of <font color="red"><strong>SIGGRAPH Asia</strong></font>, 2022</em></p>
              <ul class="list-inline">
                <li class="list-inline-item"><a href="https://arxiv.org/abs/2209.10833">Paper</a></li>
                <li class="list-inline-item"><a href="https://github.com/hu-hy17/PhysInteraction">Code</a></li>
                <li class="list-inline-item"><a href="https://www.youtube.com/watch?v=cSFWQOTuXIA">Video</a></li>
              </ul>
            </div>
          </div>
      
          <div class="media mt-4">
            <img class="mr-3 img-thumbnail" style="height:150px" src="images/TransPose.jpg" alt="TransPose">
            <div class="media-body">
              <h6 class="mt-0">TransPose: Real-time 3D Human Translation and Pose Estimation with Six Inertial Sensors</h6>
              <p><strong>Xinyu Yi</strong>, Yuxiao Zhou, Feng Xu<br>
                 <em>ACM Transactions on Graphics (Proc. of <font color="red"><strong>SIGGRAPH</strong></font>), 2021</em></p>
              <ul class="list-inline">
                <li class="list-inline-item"><a href="https://arxiv.org/abs/2105.04605">Paper</a></li>
                <li class="list-inline-item"><a href="https://github.com/Xinyu-Yi/TransPose/">Code</a></li>
                <li class="list-inline-item"><a href="https://www.youtube.com/watch?v=YWSczFkPino">Video</a></li>
                <li class="list-inline-item"><a href="https://xinyu-yi.github.io/TransPose/">Project page</a></li>
              </ul>
            </div>
          </div>
      
          <div class="media mt-4">
            <img class="mr-3 img-thumbnail" style="height:150px" src="images/HierarIK.jpg" alt="HierarIK">
            <div class="media-body">
              <h6 class="mt-0">HierarIK: Hierarchical Inverse Kinematics Solver for Human Body and Hand Pose Estimation</h6>
              <p><strong>Xinyu Yi</strong>, Yuxiao Zhou, Feng Xu<br>
                 <em>CAAI International Conference on Artificial Intelligence (<font color="red"><strong>CICAI</strong></font>), 2021 (<font color="red"><strong>Best Student Paper Finalist</strong></font>)</em></p>
              <ul class="list-inline">
                <li class="list-inline-item"><a href="https://link.springer.com/chapter/10.1007/978-3-030-93049-3_31">Paper</a></li>
                <li class="list-inline-item"><a href="https://xinyu-yi.github.io/PIP/videos/HierarIK.mp4">Video</a></li>
              </ul>
            </div>
          </div>
      
        </div>
      </div>

      <!-- Education -->
      <div class="row mt-5">
        <div class="col-md-12 col-sm-12">
          <h2>Education</h2><hr>

          <!-- Ph.D. Section -->
          <div class="container">
            <div class="row mr-2">
              <div class="col-auto">
                <b>Ph.D.</b> &nbsp; Sep 2020 - July 2025 <br>
                School of Software, Tsinghua University <br>
                Advisor: <a href="http://xufeng.site/">Prof. Feng Xu</a>
              </div>
              <div class="col text-right">
                <img src="images/tsinghua.png" class="img-fluid rounded" style="max-width: 90px;">
              </div>
            </div>
          </div>

          <!-- B.S. Section -->
          <div class="container mt-3">
            <div class="row mr-2">
              <div class="col-auto">
                <b>B.S.</b> &nbsp; Sep 2016 - July 2020 <br>
                University of Science and Technology of China <br>
                School of the Gifted Young <br>
              </div>
              <div class="col text-right">
                <img src="images/ustc.png" class="img-fluid rounded" style="max-width: 90px;">
              </div>
            </div>
          </div>
        </div>
      </div>
      
      <!-- Honors & Awards -->
      <div class="container mt-4 mb-4">
        <h2>Honors &amp; Awards</h2>
        <hr>
        <ul>
          <li>SIGGRAPH 2025 Best Paper Award</li>
          <li>Outstanding Ph.D. Dissertation of Tsinghua University 2025</li>
          <li>Outstanding Graduate of Beijing 2025</li>
          <li>University Nomination for Apple Scholars in AI/ML 2025</li>
          <li>National Scholarship 2024</li>
          <li>China Guanggu Scholarship 2023 (1st)</li>
          <li>Huawei Scholarship 2022 (1st)</li>
          <li>CVPR 2022 Best Paper Finalist</li>
          <li>Kuaishou Scholarship 2021 (1st)</li>
          <li>Outstanding Student Scholarship 2018 (2nd)</li>
          <li>Guanghua Scholarship 2017 (1st)</li>
          <li>Outstanding Student Scholarship 2016 (2nd)</li>
        </ul>
      </div>
      <hr>

      <footer class="text-center">
        <div class="container">
          <div class="row">
            <div class="col-12">
              <p>Copyright © Xinyu Yi. All rights reserved.</p>
            </div>
          </div>
        </div>
      </footer>
    </div>
    
    <script src="js/jquery-3.3.1.min.js"></script>
    <script src="js/popper.min.js"></script>
    <script src="js/bootstrap-4.3.1.js"></script>
  </body>
</html>